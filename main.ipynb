{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb56257",
   "metadata": {},
   "outputs": [],
   "source": [
    "#======================== C–û–ó–î–ê–ù–ò–ï –ë–î (–ù–ï –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û)================================#\n",
    "!apt install postgresql postgresql-contrib -y > /dev/null\n",
    "!service postgresql start\n",
    "!sudo -u postgres psql -c \"CREATE USER root WITH SUPERUSER PASSWORD 'root';\"\n",
    "!sudo -u postgres createdb -O root clearmedia\n",
    "!pip install psycopg2-binary SQLAlchemy > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f84914c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "conn = psycopg2.connect(\n",
    "    host=\"localhost\",\n",
    "    port=5432,\n",
    "    dbname=\"clearmedia\",\n",
    "    user=\"root\",\n",
    "    password=\"root\",\n",
    ")\n",
    "conn.autocommit = True\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"SELECT version();\")\n",
    "print(cur.fetchone())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedf4312",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_sql = open(\"/content/drive/MyDrive/hack_2/database_schema.sql\", encoding=\"utf-8\").read()\n",
    "cur.execute(schema_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d539f9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================–û–°–ù–û–í–ù–û–ô –ü–ê–ô–ü–õ–ê–ô–ù=================================#\n",
    "!pip install -q torch transformers spacy\n",
    "!python -m spacy download en_core_web_sm -q\n",
    "!pip install -q google-trans-new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cd8027",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "os.chdir('/content/drive/MyDrive/hack_2')\n",
    "print(f\"–†–∞–±–æ—á–∞—è –ø–∞–ø–∫–∞: {os.getcwd()}\")\n",
    "print(f\"–§–∞–π–ª—ã: {os.listdir('.')}\")\n",
    "\n",
    "required_files = [\n",
    "    'sample.csv',\n",
    "    'data_loader.py',\n",
    "    'text_preprocessor.py',\n",
    "    'translator.py',\n",
    "    'entity_extractor_ner_ensemble.py',\n",
    "    'entity_deduplicator.py',\n",
    "    'risk_classifier.py',\n",
    "    'output_formatter.py',\n",
    "    'relationship_extractor_hybrid_pro.py'\n",
    "]\n",
    "\n",
    "print(\"\\n–ü–†–û–í–ï–†–ö–ê –§–ê–ô–õ–û–í:\")\n",
    "for f in required_files:\n",
    "    status = \"‚úÖ\" if os.path.exists(f) else \"‚ùå\"\n",
    "    print(f\"{status} {f}\")\n",
    "\n",
    "print(\"\\n–ì–æ—Ç–æ–≤–æ –∫ –∑–∞–ø—É—Å–∫—É –ø–∞–π–ø–ª–∞–π–Ω–∞!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327ba1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import os\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def safe_import(module_name, class_name):\n",
    "    try:\n",
    "        module = __import__(module_name)\n",
    "        return getattr(module, class_name)()\n",
    "    except Exception as e:\n",
    "        print(f\"{module_name}.{class_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥—É–ª–µ–π...\")\n",
    "data_loader = safe_import('data_loader', 'DataLoader')\n",
    "text_preprocessor = safe_import('text_preprocessor', 'TextPreprocessor')\n",
    "translator = safe_import('translator', 'Translator') or None\n",
    "ner_extractor = safe_import('entity_extractor_ner_ensemble', 'NEREnsembleExtractor')\n",
    "entity_deduplicator = safe_import('entity_deduplicator', 'EntityDeduplicator')\n",
    "risk_classifier = safe_import('risk_classifier', 'RiskClassifier')\n",
    "output_formatter = safe_import('output_formatter', 'OutputFormatter')\n",
    "relation_extractor = safe_import('relationship_extractor_hybrid_pro', 'RelationExtractorHybridPro')\n",
    "print(\"–í—Å–µ –º–æ–¥—É–ª–∏ –≥–æ—Ç–æ–≤—ã\")\n",
    "\n",
    "def to_serializable(obj):\n",
    "    \"\"\"–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç Entity –æ–±—ä–µ–∫—Ç—ã –≤ dict –¥–ª—è JSON\"\"\"\n",
    "    if hasattr(obj, '__dict__'):\n",
    "        return obj.__dict__\n",
    "    if hasattr(obj, 'to_dict'):\n",
    "        return obj.to_dict()\n",
    "    if isinstance(obj, list):\n",
    "        return [to_serializable(i) for i in obj]\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: to_serializable(v) for k, v in obj.items()}\n",
    "    return obj\n",
    "\n",
    "class NewsAnalysisPipelineHybrid:\n",
    "    def process_article(self, article: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        raw_id = article.get('id', 'unknown')\n",
    "        article_id = str(raw_id)[:8]\n",
    "        title = str(article.get('title', 'No title'))[:60]\n",
    "        text = article.get('content', '') or article.get('text', '')\n",
    "        \n",
    "        print(f\"[{article_id}] {title}\")\n",
    "        result = {'article_id': article_id, 'title': title}\n",
    "        \n",
    "        try:\n",
    "            # 1. –ü–†–ï–î–û–ë–†–ê–ë–û–¢–ö–ê\n",
    "            cleaned_text = text\n",
    "            if text_preprocessor:\n",
    "                cleaned_text = text_preprocessor.preprocess(text) or text\n",
    "            \n",
    "            print(f\"{len(cleaned_text)} —Å–∏–º–≤–æ–ª–æ–≤\")\n",
    "            \n",
    "            # 2. NER\n",
    "            entities = {}\n",
    "            if ner_extractor:\n",
    "                ner_result = ner_extractor.extract(cleaned_text)\n",
    "                if ner_result:\n",
    "                    entities = ner_result.get(\"entities\", {})\n",
    "                    print(f\"   üë• {len(entities.get('all', []))} —Å—É—â–Ω–æ—Å—Ç–µ–π\")\n",
    "            \n",
    "            # 3. –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è\n",
    "            if entity_deduplicator and entities:\n",
    "                entities.setdefault('persons', [])\n",
    "                entities.setdefault('organizations', [])\n",
    "                entities = entity_deduplicator.deduplicate_entities(entities) or entities\n",
    "            \n",
    "            # 4. –û–¢–ù–û–®–ï–ù–ò–Ø\n",
    "            relations = []\n",
    "            if relation_extractor and entities.get('all'):\n",
    "                relations = relation_extractor.extract_relationships(cleaned_text, entities) or []\n",
    "                print(f\"   üîó {len(relations)} –æ—Ç–Ω–æ—à–µ–Ω–∏–π\")\n",
    "            \n",
    "            # 5. Knowledge Graph\n",
    "            knowledge_graph = {\n",
    "                'nodes': [],\n",
    "                'edges': []\n",
    "            }\n",
    "            \n",
    "            # Nodes (–ø–æ–¥–¥–µ—Ä–∂–∫–∞ Entity –∏ dict)\n",
    "            for entity in entities.get('all', []):\n",
    "                if hasattr(entity, 'name'):\n",
    "                    name = entity.name\n",
    "                    etype = getattr(entity, 'entity_type', getattr(entity, 'type', 'UNKNOWN'))\n",
    "                elif isinstance(entity, dict):\n",
    "                    name = entity.get('name', str(entity))\n",
    "                    etype = entity.get('type', entity.get('entity_type', 'UNKNOWN'))\n",
    "                else:\n",
    "                    name = str(entity)\n",
    "                    etype = 'UNKNOWN'\n",
    "                \n",
    "                knowledge_graph['nodes'].append({\n",
    "                    'id': name,\n",
    "                    'type': etype,\n",
    "                    'name': name\n",
    "                })\n",
    "            \n",
    "            # Edges\n",
    "            for rel in relations:\n",
    "                if hasattr(rel, 'to_dict'):\n",
    "                    knowledge_graph['edges'].append(rel.to_dict())\n",
    "                elif isinstance(rel, dict):\n",
    "                    knowledge_graph['edges'].append(rel)\n",
    "            \n",
    "            result.update({\n",
    "                'entities': to_serializable(entities), \n",
    "                'knowledge_graph': knowledge_graph,\n",
    "                'success': True\n",
    "            })\n",
    "            \n",
    "            print(f\"{len(knowledge_graph['nodes'])} nodes | {len(knowledge_graph['edges'])} edges\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            result['error'] = str(e)\n",
    "            print(f\"{str(e)[:60]}\")\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def run(self, max_articles: int = 10):\n",
    "        print(\"\\n–ó–ê–ü–£–°–ö –ê–ù–ê–õ–ò–ó–ê\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # –ó–∞–≥—Ä—É–∑–∫–∞\n",
    "        articles = []\n",
    "        try:\n",
    "            if data_loader:\n",
    "                all_articles = data_loader.load('sample.csv')\n",
    "                articles = all_articles[:max_articles]\n",
    "        except:\n",
    "            print(\"Fallback –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã–µ\")\n",
    "            articles = [{'id': i, 'title': f'Test {i}', 'content': 'Ilham Aliyev CEO SOCAR Baku.'} for i in range(max_articles)]\n",
    "        \n",
    "        print(f\"üìÑ {len(articles)} —Å—Ç–∞—Ç–µ–π\")\n",
    "        \n",
    "        results = []\n",
    "        for i, article in enumerate(articles, 1):\n",
    "            result = self.process_article(article)\n",
    "            results.append(result)\n",
    "            print(f\"[{i}/{len(articles)}] {'‚úÖ' if result.get('success') else '‚ùå'}\")\n",
    "        \n",
    "        # JSON‚Äë—Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏—è\n",
    "        serializable_results = to_serializable(results)\n",
    "        \n",
    "        # –°–û–•–†–ê–ù–ï–ù–ò–ï\n",
    "        output_file = 'results_hybrid_final.json'\n",
    "        print(f\"\\n–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ {output_file}...\")\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(serializable_results, f, ensure_ascii=False, indent=2)\n",
    "        print(\"–§–ê–ô–õ –ì–û–¢–û–í!\")\n",
    "        \n",
    "        # –°–¢–ê–¢–ò–°–¢–ò–ö–ê\n",
    "        successful = sum(1 for r in results if r.get('success'))\n",
    "        total_relations = sum(len(r.get('knowledge_graph', {}).get('edges', [])) for r in results)\n",
    "        \n",
    "        print(f\"\\n–†–ï–ó–£–õ–¨–¢–ê–¢–´:\")\n",
    "        print(f\"–£—Å–ø–µ—à–Ω–æ: {successful}/{len(results)}\")\n",
    "        print(f\"–û—Ç–Ω–æ—à–µ–Ω–∏–π: {total_relations}\")\n",
    "        \n",
    "        if total_relations > 0:\n",
    "            print(\"\\n–¢–û–ü-5 –û–¢–ù–û–®–ï–ù–ò–ô:\")\n",
    "            all_edges = []\n",
    "            for r in results:\n",
    "                all_edges.extend(r.get('knowledge_graph', {}).get('edges', []))\n",
    "            \n",
    "            for edge in sorted(all_edges, key=lambda x: x.get('confidence', 0), reverse=True)[:5]:\n",
    "                print(f\"   {edge.get('source_entity', '?')} ‚Üí [{edge.get('relation_type', '?')}] ‚Üí {edge.get('target_entity', '?')} ({edge.get('confidence', 0):.2f})\")\n",
    "        \n",
    "        print(f\"\\nresults_hybrid_final.json ‚Äî –ì–û–¢–û–í–û!\")\n",
    "        return serializable_results\n",
    "\n",
    "# –§–ò–ù–ê–õ–¨–ù–´–ô –ó–ê–ü–£–°–ö\n",
    "pipeline = NewsAnalysisPipelineHybrid()\n",
    "results = pipeline.run(max_articles=3)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
