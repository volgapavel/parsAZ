## Gold-набор для тестирования (50×3)

### Как брали
- **Источники**: `01.csv → report.az`, `02.csv → azerbaijan.az`, `03.csv → trend.az`.
- **Размер**: строго **50 статей на источник** (**150 total**).
- **Кандидаты**: брали из **первых `--read-limit` строк** каждого CSV (быстро и воспроизводимо).
- **Фильтр качества**: оставляли только статьи, где:
  - **`id`** не пустой
  - **`title`** длиной **≥ 5**
  - **`content`** длиной **≥ 200**
- **Исключение уже размеченных**: убрали статьи, которые уже были размечены вручную (`manually_verified=true`) в `evaluation/gold/gold_dataset.json`.
- **Разнообразие**: кандидатов сортировали по длине `content` и брали **равномерно по шкале длины** (квантили) + **случайный добор**, если не хватало из‑за дублей.
- **Строгость**: `--strict` гарантирует, что получится ровно 50 на источник (иначе нужно увеличить `--read-limit`).

### Почему именно так
- **Скорость**: быстро получить рабочий тестовый набор без ручного тематического отбора.
- **Баланс**: 50/50/50 убирает перекос по источникам и стилям новостей.
- **Стресс‑тест пайплайна**: разная длина текста = разная “сложность” (короткие/средние/длинные).
- **Чистый раунд разметки**: исключение уже размеченных даёт независимую новую выборку для следующей проверки.
- **Воспроизводимость**: фиксированные правила + `seed` → можно повторять и сравнивать версии модели/пайплайна.

### На что обращали внимание
- **Качество текста** (непустой заголовок и содержимое, отсев мусора).
- **Разнообразие форматов** (покрытие по длине `content`).
- **Отсутствие пересечений** с уже размеченными статьями.
- **Уникальность** по `(source, article_id)` (без дублей в наборе).

### Что получилось
- **Файл**: `evaluation/gold/gold_dataset_50x3_unlabeled.json`
- **Объём**: 150 статей (50/50/50)
- **Разметка**: `manually_verified=true` — **0** (набор новый, неразмеченный)

### Что ещё можно сказать про подход к сбору (для отчёта/презентации)
- **Плюсы**:
  - воспроизводимый отбор (фиксированные правила и `seed`)
  - баланс источников
  - базовый quality gate до NLP
  - разнообразие по длине как простой прокси сложности
- **Ограничения**:
  - выбор из первых `--read-limit` строк может давать сдвиг по времени/разделам, если CSV отсортирован
  - длина текста ≠ гарантированное покрытие risk-тем (санкции/коррупция/суды)
- **Как усилить дальше**:
  - стратифицировать выборку по **времени/разделам/ключевым словам риска**
  - дедуп делать не только по `id`, но и по **URL/хешу контента**
  - сохранять метаданные парсинга (дата, источник, раздел, длина, хеш) для аудита качества данных