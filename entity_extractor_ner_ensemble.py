"""
entity_extractor_ner_ensemble.py

Ensemble NER extractor –¥–ª—è –∞–∑–µ—Ä–±–∞–π–¥–∂–∞–Ω—Å–∫–æ–≥–æ —è–∑—ã–∫–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º:
1. Davlan/xlm-roberta-large-ner-hrl (–æ—Å–Ω–æ–≤–Ω–∞—è –º–æ–¥–µ–ª—å: PER, ORG, LOC)
2. LocalDoc/private_ner_azerbaijani_v2 (–¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è: –¥–∞—Ç—ã, –¥–æ–ª–∂–Ω–æ—Å—Ç–∏, –∞–¥—Ä–µ—Å–∞)

–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:
- –î–≤–æ–π–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –æ—Å–Ω–æ–≤–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π (PER, ORG, LOC)
- –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ —Ç–∏–ø—ã —Å—É—â–Ω–æ—Å—Ç–µ–π (–¥–∞—Ç—ã, —Å–æ–±—ã—Ç–∏—è, –¥–æ–ª–∂–Ω–æ—Å—Ç–∏)
- –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Å–≤—è–∑–µ–π –º–µ–∂–¥—É —Å—É—â–Ω–æ—Å—Ç—è–º–∏ —á–µ—Ä–µ–∑ regex + –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω—ã–π –ø–æ–¥—Ö–æ–¥
"""

import re
import logging
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass, asdict, field
from collections import defaultdict

import torch
from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline

logger = logging.getLogger(__name__)


@dataclass
class Entity:
    """–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–∏"""
    name: str
    entity_type: str  # person, organization, location, position, date, event
    confidence: float
    context: str
    span: Tuple[int, int]  # (start, end)
    source: str  # 'davlan' –∏–ª–∏ 'localdoc' –∏–ª–∏ 'ensemble'
    
    def to_dict(self):
        return {
            "name": self.name,
            "confidence": float(self.confidence),  # üî• float32 ‚Üí float
            "context": self.context,
            "type": self.entity_type,
            "source": self.source
        }

@dataclass
class Relationship:
    """–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Å–≤—è–∑–∏ –º–µ–∂–¥—É —Å—É—â–Ω–æ—Å—Ç—è–º–∏"""
    source_entity: str
    target_entity: str
    relation_type: str  # works_for, located_in, manages, owns, etc
    confidence: float
    evidence: str  # —Ç–µ–∫—Å—Ç, –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞—é—â–∏–π —Å–≤—è–∑—å
    source: str  # regex, pattern, contextual


class NEREnsembleExtractor:
    """
    Ensemble NER extractor –¥–ª—è –∞–∑–µ—Ä–±–∞–π–¥–∂–∞–Ω—Å–∫–æ–≥–æ.
    
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–≤–µ –¥–æ–ø–æ–ª–Ω—è—é—â–∏–µ –¥—Ä—É–≥ –¥—Ä—É–≥–∞ BERT-–º–æ–¥–µ–ª–∏:
    1. Davlan - —à–∏—Ä–æ–∫–æ–µ –ø–æ–∫—Ä—ã—Ç–∏–µ –æ—Å–Ω–æ–≤–Ω—ã—Ö —Ç–∏–ø–æ–≤
    2. LocalDoc - —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–∞ az
    """
    
    def __init__(self, device: str = "cuda" if torch.cuda.is_available() else "cpu"):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –æ–±–µ–∏—Ö –º–æ–¥–µ–ª–µ–π.
        
        Args:
            device: 'cuda' –∏–ª–∏ 'cpu'
        """
        self.device = device
        logger.info(f"üîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è NER Ensemble –Ω–∞ {device}...")
        
        # 1Ô∏è‚É£ –û—Å–Ω–æ–≤–Ω–∞—è –º–æ–¥–µ–ª—å: Davlan (3 —Ç–∏–ø–∞: PER, ORG, LOC)
        try:
            self.davlan_model_name = "Davlan/xlm-roberta-large-ner-hrl"
            self.davlan_tokenizer = AutoTokenizer.from_pretrained(self.davlan_model_name)
            self.davlan_model = AutoModelForTokenClassification.from_pretrained(
                self.davlan_model_name
            ).to(self.device)
            self.davlan_nlp = pipeline(
                "ner",
                model=self.davlan_model,
                tokenizer=self.davlan_tokenizer,
                device=0 if device == "cuda" else -1,
                aggregation_strategy="simple"
            )
            logger.info("‚úÖ Davlan –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ Davlan: {e}")
            self.davlan_nlp = None
        
        # 2Ô∏è‚É£ –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å: LocalDoc (25 —Ç–∏–ø–æ–≤, az-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω–∞—è)
        try:
            self.localdoc_model_name = "LocalDoc/private_ner_azerbaijani_v2"
            self.localdoc_tokenizer = AutoTokenizer.from_pretrained(self.localdoc_model_name)
            self.localdoc_model = AutoModelForTokenClassification.from_pretrained(
                self.localdoc_model_name
            ).to(self.device)
            self.localdoc_nlp = pipeline(
                "ner",
                model=self.localdoc_model,
                tokenizer=self.localdoc_tokenizer,
                device=0 if device == "cuda" else -1,
                aggregation_strategy="simple"
            )
            logger.info("‚úÖ LocalDoc –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è LocalDoc –º–æ–¥–µ–ª—å –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞ (opt): {e}")
            self.localdoc_nlp = None
        
        # –°–ª–æ–≤–∞—Ä–∏ –¥–ª—è post-processing
        self.position_keywords = {
            "az": [
                "prezidenti", "ba≈ü naziri", "nazir", "m√ºd√ºr", "direktor",
                "s…ôfir", "m…ôsl…ôh…ôt√ßi", "h√∂kum…ôti", "r…ôis", "ba≈ü√ßƒ±",
                "tap≈üƒ±rƒ±q√ßƒ±", "koordinator", "assistent", "≈üef", "m…ônas",
                "n√ºmay…ônd…ôsi", "agenti", "…ôm…ôkda≈üƒ±"
            ],
            "ru": [
                "–ø—Ä–µ–∑–∏–¥–µ–Ω—Ç", "–ø—Ä–µ–º—å–µ—Ä-–º–∏–Ω–∏—Å—Ç—Ä", "–º–∏–Ω–∏—Å—Ç—Ä", "–¥–∏—Ä–µ–∫—Ç–æ—Ä",
                "–ø–æ—Å–æ–ª", "—Å–æ–≤–µ—Ç–Ω–∏–∫", "–≥–ª–∞–≤–∞", "–≥–ª–∞–≤–∞ –∫–æ–º–ø–∞–Ω–∏–∏", "—Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª—å"
            ]
        }
        
        self.location_keywords = {
            "az": [
                "bakƒ±", "g…ônc…ô", "sumqayƒ±t", "q…ôb…ôl…ô", "shamakhi", 
                "pol≈üa", "b√∂y√ºk britaniya", "ab≈ü", "misir", "suriy…ô", 
                "t√ºrkiy…ô", "irran", "rusiya", "√ßin", "hindistan",
                "fransiya", "almaniya", "italiya", "ispaniya", "portekiz"
            ],
            "en": [
                "poland", "britain", "united states", "egypt", "syria",
                "turkey", "iran", "russia", "china", "india",
                "france", "germany", "italy", "spain", "portugal"
            ]
        }
    def _extract_positions_from_context(self, entities: Dict, text: str) -> List:
      """–ò–∑–≤–ª–µ–∫–∞–µ–º –¥–æ–ª–∂–Ω–æ—Å—Ç–∏ –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Å—É—â–Ω–æ—Å—Ç–µ–π"""
      positions = []
      person_names = [p.name for p in entities.get('persons', [])]
      
      # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –¥–æ–ª–∂–Ω–æ—Å—Ç–µ–π
      position_patterns = [
          rf'({re.escape(person)}),\s*([\w\d\-]+\s+[\w\d\-]+(?:\s+[\w\d\-])?)' 
          for person in person_names[:5]  # –¢–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 5 –ø–µ—Ä—Å–æ–Ω
      ]
      
      for pattern in position_patterns:
          for match in re.finditer(pattern, text):
              position = match.group(2).strip()
              # –§–∏–ª—å—Ç—Ä—É–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ –∏ –æ–±—â–∏–µ —Å–ª–æ–≤–∞
              if len(position) > 4 and position not in ['dedik', 'olub', 'edib']:
                  positions.append(Entity(
                      name=position,
                      entity_type='position',
                      confidence=0.7,
                      context=match.group(0),
                      span=(match.start(), match.end()),
                      source='pattern'
                  ))
      
      return positions

    def extract_entities_davlan(self, text: str) -> List[Entity]:
      """üîß –§–ò–ö–°: Davlan extraction —Å –¥–µ–±–∞–≥–æ–º"""
      if not self.davlan_nlp:
          return []
      
      entities = []
      try:
          ner_results = self.davlan_nlp(text)
          print(f"üîç Davlan raw result: {ner_results[:2] if ner_results else 'EMPTY'}")  # –î–ï–ë–ê–ì
          
          for result in ner_results:
              # üî• –§–ò–ö–°: –ø—Ä–æ–≤–µ—Ä—è–µ–º –≤—Å–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ –∫–ª—é—á–∏
              entity_key = None
              for key in ['entity', 'entity_group', 'label', 'type']:
                  if key in result:
                      entity_key = key
                      break
              
              if not entity_key:
                  logger.warning(f"–ù–µ—Ç entity –∫–ª—é—á–∞: {result.keys()}")
                  continue
                  
              entity_type_raw = result[entity_key].replace('B-', '').replace('I-', '').lower()
              
              type_mapping = {'per': 'person', 'org': 'organization', 'loc': 'location'}
              entity_type = type_mapping.get(entity_type_raw, entity_type_raw)
              
              entity = Entity(
                  name=result['word'].strip(),
                  entity_type=entity_type,
                  confidence=float(result.get('score', 0.5)),
                  context=text[max(0, result.get('start', 0)-50):result.get('end', len(text))+50],
                  span=(int(result.get('start', 0)), int(result.get('end', 0))),
                  source='davlan'
              )
              entities.append(entity)
              
      except Exception as e:
          logger.error(f"–û—à–∏–±–∫–∞ –≤ Davlan: {e}")
      
      return entities

    def extract_entities_localdoc(self, text: str) -> List[Entity]:
        """üîß –§–ò–ö–°: LocalDoc extraction"""
        if not self.localdoc_nlp:
            return []
        
        entities = []
        try:
            ner_results = self.localdoc_nlp(text)
            print(f"üîç LocalDoc raw: {ner_results[:2] if ner_results else 'EMPTY'}")  # –î–ï–ë–ê–ì
            
            for result in ner_results:
                entity_key = None
                for key in ['entity', 'entity_group', 'label', 'type']:
                    if key in result:
                        entity_key = key
                        break
                
                if not entity_key:
                    continue
                    
                entity_type_raw = result[entity_key].replace('B-', '').replace('I-', '').lower()
                
                type_mapping = {
                    'givenname': 'person', 'surname': 'person', 'firstname': 'person', 'lastname': 'person',
                    'city': 'location', 'location': 'location',
                    'date': 'date', 'time': 'date', 'age': 'date',
                    'organization': 'organization', 'org': 'organization'
                }
                
                entity_type = type_mapping.get(entity_type_raw, entity_type_raw)
                
                entity = Entity(
                    name=result['word'].strip(),
                    entity_type=entity_type,
                    confidence=float(result.get('score', 0.5)),
                    context=text[max(0, result.get('start', 0)-50):result.get('end', len(text))+50],
                    span=(int(result.get('start', 0)), int(result.get('end', 0))),
                    source='localdoc'
                )
                entities.append(entity)
                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≤ LocalDoc: {e}")
        
        return entities
    
    def extract_relationships(self, text: str, entities: Dict[str, List[Entity]]) -> List[Relationship]:
        """
        –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Å–≤—è–∑–µ–π –º–µ–∂–¥—É —Å—É—â–Ω–æ—Å—Ç—è–º–∏.
        
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç regex –ø–∞—Ç—Ç–µ—Ä–Ω—ã + –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑.
        """
        relationships = []
        
        # 1. –ü–∞—Ç—Ç–µ—Ä–Ω: "—Å—Ç—Ä–∞–Ω–∞ prezidenti –§–ò–û" -> –§–ò–û works_for —Å—Ç—Ä–∞–Ω–∞
        pattern_country_leader = r'(\w+\s+\w*)\s+prezidenti\s+([–ê-–Ø–ï“ö“ù“π“ì“≥“∑“¢“Æ“™“é“∞“≤“∂“¥“∏“∂“º“π“æ“ø][–∞-—è–µ“õ“ù“π“ì“≥“∑“¢“Æ“™“é“∞“≤“∂“¥“∏“∂“º“π“æ“ø\s]+)'
        
        for match in re.finditer(pattern_country_leader, text, re.IGNORECASE):
            country = match.group(1).strip()
            person = match.group(2).strip()
            
            relationships.append(Relationship(
                source_entity=person,
                target_entity=country,
                relation_type="works_for",
                confidence=0.85,
                evidence=match.group(0),
                source="regex_leader"
            ))
        
        # 2. –ü–∞—Ç—Ç–µ—Ä–Ω: "–§–ò–û, <–¥–æ–ª–∂–Ω–æ—Å—Ç—å> <–æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏>" -> –§–ò–û works_for –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è
        pattern_person_role = r'([–ê-–Ø–ï“ö“ù“π“ì“≥“∑“¢“Æ“™“é“∞“≤“∂“¥“∏“∂“º“π“æ“ø][–∞-—è–µ“õ“ù“π“ì“≥“∑“¢“Æ“™“é“∞“≤“∂“¥“∏“∂“º“π“æ“ø\s]+),\s+([\w”ô“π“ì“≥“∑“û“¢“Æ“™“é“∞“≤“∂“¥“∏“∂“º“π“æ“ø\s]+)(?:\s+(?:–∫–æ–º–ø–∞–Ω–∏—è—Å–∏–¥”ô|–±–∞–Ω–∫–∏–¥–∞|–º–∏–Ω–∏—Å—Çr…ôsind…ô|≈üirk…ôtind…ô|m…ôs…ôl…ôsind…ô))'
        
        for match in re.finditer(pattern_person_role, text, re.IGNORECASE):
            person = match.group(1).strip()
            position = match.group(2).strip()
            
            relationships.append(Relationship(
                source_entity=person,
                target_entity=position,
                relation_type="has_position",
                confidence=0.8,
                evidence=match.group(0),
                source="regex_position"
            ))
        
        # 3. –ü–∞—Ç—Ç–µ—Ä–Ω: "X v…ô Y ≈üirk…ôtl…ôri" -> X, Y –≤ –æ–¥–Ω–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        pattern_companies = r'(\w+)\s+(?:v…ô|—ñ—Å|–∏)\s+(\w+)\s+(?:≈üirk…ôtl…ôri|kompaniyalarƒ±|banklarƒ±)'
        
        for match in re.finditer(pattern_companies, text, re.IGNORECASE):
            entity1 = match.group(1).strip()
            entity2 = match.group(2).strip()
            
            relationships.append(Relationship(
                source_entity=entity1,
                target_entity=entity2,
                relation_type="competitors",
                confidence=0.75,
                evidence=match.group(0),
                source="regex_competitors"
            ))
        
        return relationships
    
    def extract(self, text: str) -> Dict[str, Any]:
        """
        –ü–æ–ª–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π –∏ —Å–≤—è–∑–µ–π.
        """
        # 1. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —á–µ—Ä–µ–∑ –æ–±–µ –º–æ–¥–µ–ª–∏
        davlan_entities = self.extract_entities_davlan(text)
        localdoc_entities = self.extract_entities_localdoc(text)

        # 2. –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –∏ –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è
        all_entities = self._merge_entities(davlan_entities, localdoc_entities)

        # 3. –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ —Ç–∏–ø–∞–º
        grouped_entities = self._group_entities(all_entities)

        # 4. üî• –ò–ó–í–õ–ï–ß–ï–ù–ò–ï –î–û–õ–ñ–ù–û–°–¢–ï–ô
        position_entities = self._extract_positions_from_context(grouped_entities, text)
        grouped_entities['position'] = position_entities

        # 5. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–≤—è–∑–µ–π
        relationships = self.extract_relationships(text, grouped_entities)

        # 6. –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∞ –∑–Ω–∞–Ω–∏–π
        kg_nodes, kg_edges = self._build_knowledge_graph(grouped_entities, relationships)

        return {
            "entities": {
                "persons": grouped_entities.get('person', []),
                "organizations": grouped_entities.get('organization', []),
                "locations": grouped_entities.get('location', []),
                "dates": grouped_entities.get('date', []),
                "events": grouped_entities.get('event', []),
                "positions": grouped_entities.get('position', []),
                "all": all_entities,
            },
            "relationships": [asdict(r) for r in relationships],
            "knowledge_graph": {
                "nodes": kg_nodes,
                "edges": kg_edges,
            },
        }
    
    def _merge_entities(self, davlan: List[Entity], localdoc: List[Entity]) -> List[Entity]:
        """üîß –§–ò–ö–°: –±–µ–∑–æ–ø–∞—Å–Ω–æ–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ"""
        merged = {}
        
        all_entities = davlan + localdoc  # –ü—Ä–æ—Å—Ç–æ –∫–æ–Ω–∫–∞—Ç–µ–Ω–∏—Ä—É–µ–º
        
        for ent in all_entities:
            # üî• –§–ò–ö–°: –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Ç–∏–ø—ã
            type_map = {
                'per': 'person', 'givenname': 'person', 'surname': 'person',
                'firstname': 'person', 'lastname': 'person',
                'org': 'organization', 'organization': 'organization',
                'loc': 'location', 'city': 'location', 'location': 'location',
                'date': 'date', 'time': 'date', 'age': 'date'
            }
            norm_type = type_map.get(ent.entity_type.lower(), ent.entity_type)
            
            key = (ent.name.lower().strip(), norm_type)
            
            if key not in merged or ent.confidence > merged[key].confidence:
                merged[key] = ent
                merged[key].entity_type = norm_type  # ‚Üê –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Ç–∏–ø
        
        return list(merged.values())
    
    def _group_entities(self, entities: List[Entity]) -> Dict[str, List[Dict]]:
        """–ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ —Å—É—â–Ω–æ—Å—Ç–µ–π –ø–æ —Ç–∏–ø–∞–º"""
        grouped = defaultdict(list)
        
        for ent in entities:
            grouped[ent.entity_type].append(ent.to_dict())
        
        return dict(grouped)
    
    def _build_knowledge_graph(
        self, 
        grouped_entities: Dict[str, List], 
        relationships: List[Relationship]
    ) -> Tuple[Dict, List]:
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∞ –∑–Ω–∞–Ω–∏–π –∏–∑ —Å—É—â–Ω–æ—Å—Ç–µ–π –∏ —Å–≤—è–∑–µ–π"""
        
        nodes = {
            "persons": grouped_entities.get('person', []),
            "organizations": grouped_entities.get('organization', []),
            "locations": grouped_entities.get('location', []),
            "dates": grouped_entities.get('date', []),
            "events": grouped_entities.get('event', []),
            "positions": grouped_entities.get('position', []),
        }
        
        edges = [
            {
                "source": r.source_entity,
                "target": r.target_entity,
                "type": r.relation_type,
                "confidence": r.confidence,
                "evidence": r.evidence
            }
            for r in relationships
        ]
        
        return nodes, edges
