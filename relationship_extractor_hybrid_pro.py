"""
relationship_extractor_hybrid_pro.py

ÐŸÐ¾Ð»Ð½Ð¾Ñ„ÑƒÐ½ÐºÑ†Ð¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ð³Ð¸Ð±Ñ€Ð¸Ð´Ð½Ñ‹Ð¹ ÑÐºÑÑ‚Ñ€Ð°ÐºÑ‚Ð¾Ñ€ Ð¾Ñ‚Ð½Ð¾ÑˆÐµÐ½Ð¸Ð¹ Ð¼ÐµÐ¶Ð´Ñƒ ÑÑƒÑ‰Ð½Ð¾ÑÑ‚ÑÐ¼Ð¸

Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ 3 Ð¼ÐµÑ‚Ð¾Ð´Ð° Ñ graceful fallback:
1. Regex Ð¿Ð°Ñ‚Ñ‚ÐµÑ€Ð½Ñ‹ (85% Ñ‚Ð¾Ñ‡Ð½Ð¾ÑÑ‚ÑŒ, Ð±Ñ‹ÑÑ‚Ñ€Ð¾)
2. spaCy Dependency Parsing (72% Ñ‚Ð¾Ñ‡Ð½Ð¾ÑÑ‚ÑŒ, Ð¸Ð½Ñ‚ÐµÑ€Ð¿Ñ€ÐµÑ‚Ð¸Ñ€ÑƒÐµÐ¼Ð¾)
3. Zero-shot BERT Classification (80-85% Ñ‚Ð¾Ñ‡Ð½Ð¾ÑÑ‚ÑŒ, ÑƒÐ½Ð¸Ð²ÐµÑ€ÑÐ°Ð»ÑŒÐ½Ð¾)
"""

import logging
import re
from typing import List, Dict, Any, Tuple, Optional
from dataclasses import dataclass, field
from collections import defaultdict

logger = logging.getLogger(__name__)


@dataclass
class ExtractedRelation:
    """ÐœÐ¾Ð´ÐµÐ»ÑŒ Ð´Ð»Ñ Ð¸Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð½Ð¾Ð³Ð¾ Ð¾Ñ‚Ð½Ð¾ÑˆÐµÐ½Ð¸Ñ"""
    source_entity: str
    target_entity: str
    relation_type: str
    confidence: float
    evidence: str
    source_method: str  # 'regex', 'spacy', 'bert', 'entity_linking'
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            'source_entity': self.source_entity,
            'target_entity': self.target_entity,
            'relation_type': self.relation_type,
            'confidence': float(self.confidence),
            'evidence': self.evidence,
            'source_method': self.source_method
        }


class RelationExtractorHybridPro:
    """
    Ð“Ð¸Ð±Ñ€Ð¸Ð´Ð½Ñ‹Ð¹ ÑÐºÑÑ‚Ñ€Ð°ÐºÑ‚Ð¾Ñ€ Ñ 3 Ð¼ÐµÑ‚Ð¾Ð´Ð°Ð¼Ð¸ Ð¸ graceful fallback
    
    ÐÑ€Ñ…Ð¸Ñ‚ÐµÐºÑ‚ÑƒÑ€Ð°:
    - Regex: BASE LAYER (Ð²ÑÐµÐ³Ð´Ð° Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÐµÑ‚)
    - spaCy: SYNTAX LAYER (ÐµÑÐ»Ð¸ Ð´Ð¾ÑÑ‚ÑƒÐ¿ÐµÐ½)
    - BERT: SEMANTIC LAYER (ÐµÑÐ»Ð¸ Ð´Ð¾ÑÑ‚ÑƒÐ¿ÐµÐ½)
    """
    
    def __init__(
        self,
        use_regex: bool = True,
        use_spacy: bool = True,
        use_bert: bool = True,
        device: str = "cpu"
    ):
        """
        Args:
            use_regex: Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÑŒ regex Ð¿Ð°Ñ‚Ñ‚ÐµÑ€Ð½Ñ‹
            use_spacy: ÐŸÑ‹Ñ‚Ð°Ñ‚ÑŒÑÑ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÑŒ spaCy
            use_bert: ÐŸÑ‹Ñ‚Ð°Ñ‚ÑŒÑÑ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÑŒ BERT
            device: 'cpu' Ð¸Ð»Ð¸ 'cuda'
        """
        self.device = device
        self.methods_loaded = []
        
        # Layer 1: Regex (Ð’Ð¡Ð•Ð“Ð”Ð Ð¸Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐµÑ‚ÑÑ)
        if use_regex:
            self._init_regex()
        
        # Layer 2: spaCy (Ñ graceful fallback)
        if use_spacy:
            self._init_spacy()
        
        # Layer 3: BERT Zero-shot (Ñ graceful fallback)
        if use_bert:
            self._init_bert()
        
        logger.info(f"âœ… Loaded methods: {', '.join(self.methods_loaded)}")
    
    def _init_regex(self) -> None:
        """Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Regex Ð¼ÐµÑ‚Ð¾Ð´Ð°"""
        try:
            self.regex_patterns = self._compile_patterns()
            self.methods_loaded.append('regex')
            logger.info("âœ… Regex patterns compiled")
        except Exception as e:
            logger.error(f"âŒ Failed to init regex: {e}")
    
    def _init_spacy(self) -> None:
        """Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ spaCy Ñ graceful fallback"""
        try:
            import spacy
            try:
                self.spacy_nlp = spacy.load("en_core_web_sm")
                self.methods_loaded.append('spacy')
                logger.info("âœ… spaCy loaded (en_core_web_sm)")
            except OSError:
                logger.warning("âš ï¸ spaCy model not found, downloading...")
                import subprocess
                subprocess.run(
                    ["python", "-m", "spacy", "download", "en_core_web_sm"],
                    capture_output=True
                )
                self.spacy_nlp = spacy.load("en_core_web_sm")
                self.methods_loaded.append('spacy')
                logger.info("âœ… spaCy loaded (en_core_web_sm)")
        except ImportError:
            logger.warning("âš ï¸ spaCy not installed, skipping syntax layer")
            self.spacy_nlp = None
        except Exception as e:
            logger.warning(f"âš ï¸ spaCy init failed: {e}")
            self.spacy_nlp = None
    
    def _init_bert(self) -> None:
        """Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ BERT Zero-shot Ñ graceful fallback"""
        try:
            from transformers import pipeline
            import torch
            
            # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ð¾ÑÑ‚ÑŒ CUDA
            if self.device == "cuda" and not torch.cuda.is_available():
                logger.warning("âš ï¸ CUDA not available, using CPU")
                device = 0 if torch.cuda.is_available() else -1
            else:
                device = 0 if self.device == "cuda" else -1
            
            self.zeroshot_pipeline = pipeline(
                "zero-shot-classification",
                model="facebook/bart-large-mnli",
                device=device
            )
            self.methods_loaded.append('bert')
            logger.info("âœ… BERT zero-shot pipeline loaded")
        except ImportError:
            logger.warning("âš ï¸ Transformers not installed, skipping BERT layer")
            self.zeroshot_pipeline = None
        except Exception as e:
            logger.warning(f"âš ï¸ BERT init failed: {e}")
            self.zeroshot_pipeline = None
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Ð“Ð›ÐÐ’ÐÐ«Ð™ ÐœÐ•Ð¢ÐžÐ”
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def extract_relationships(
        self,
        text: str,
        entities: Dict[str, List[Any]]
    ) -> List[ExtractedRelation]:
        """
        Ð“Ð»Ð°Ð²Ð½Ñ‹Ð¹ Ð¼ÐµÑ‚Ð¾Ð´ - Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ Ð²ÑÐµ Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ñ‹Ðµ Ð¼ÐµÑ‚Ð¾Ð´Ñ‹
        
        Args:
            text: Ð¢ÐµÐºÑÑ‚ ÑÑ‚Ð°Ñ‚ÑŒÐ¸
            entities: Dict Ñ ÐºÐ»ÑŽÑ‡Ð°Ð¼Ð¸ 'persons', 'organizations', 'locations'
                     Ð—Ð½Ð°Ñ‡ÐµÐ½Ð¸Ñ: List[Dict] Ð¸Ð»Ð¸ List[str]
        
        Returns:
            List[ExtractedRelation] Ð¾Ñ‚ÑÐ¾Ñ€Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ðµ Ð¿Ð¾ confidence
        """
        all_relations = []
        
        # ÐœÐµÑ‚Ð¾Ð´ 1: Regex (Ð’Ð¡Ð•Ð“Ð”Ð)
        try:
            regex_rels = self._extract_by_regex(text, entities)
            all_relations.extend(regex_rels)
            logger.debug(f"âœ… Regex: {len(regex_rels)} relations")
        except Exception as e:
            logger.error(f"âŒ Regex extraction failed: {e}")
        
        # ÐœÐµÑ‚Ð¾Ð´ 2: spaCy (ÐµÑÐ»Ð¸ Ð´Ð¾ÑÑ‚ÑƒÐ¿ÐµÐ½)
        if self.spacy_nlp:
            try:
                spacy_rels = self._extract_by_spacy(text, entities)
                all_relations.extend(spacy_rels)
                logger.debug(f"âœ… spaCy: {len(spacy_rels)} relations")
            except Exception as e:
                logger.warning(f"âš ï¸ spaCy extraction failed: {e}")
        
        # ÐœÐµÑ‚Ð¾Ð´ 3: BERT (ÐµÑÐ»Ð¸ Ð´Ð¾ÑÑ‚ÑƒÐ¿ÐµÐ½)
        if self.zeroshot_pipeline:
            try:
                bert_rels = self._extract_by_bert(text, entities)
                all_relations.extend(bert_rels)
                logger.debug(f"âœ… BERT: {len(bert_rels)} relations")
            except Exception as e:
                logger.warning(f"âš ï¸ BERT extraction failed: {e}")
        
        # Ð”ÐµÐ´ÑƒÐ¿Ð»Ð¸ÐºÐ°Ñ†Ð¸Ñ Ð¸ ÑÐ¾Ñ€Ñ‚Ð¸Ñ€Ð¾Ð²ÐºÐ°
        final_relations = self._deduplicate_relations(all_relations)
        final_relations.sort(key=lambda x: x.confidence, reverse=True)
        
        logger.info(f"ðŸ”— Total relations: {len(final_relations)}")
        return final_relations
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ÐœÐ•Ð¢ÐžÐ” 1: REGEX ÐŸÐÐ¢Ð¢Ð•Ð ÐÐ«
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def _compile_patterns(self) -> Dict[str, Any]:
        """ÐšÐ¾Ð¼Ð¿Ð¸Ð»Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ regex Ð¿Ð°Ñ‚Ñ‚ÐµÑ€Ð½Ð¾Ð²"""
        return {
            'works_for': [
                r'(\w+(?:\s+\w+)?)\s+(?:is|was|becomes|remains)\s+(?:the\s+)?(?:CEO|director|head|founder|president|minister|ambassador)\s+(?:of|at)\s+([A-Z]\w+(?:\s+[A-Z]\w+)*)',
                r'(\w+(?:\s+\w+)?)\s+(?:work|works|worked)\s+(?:as|for)\s+([A-Z]\w+(?:\s+[A-Z]\w+)*)',
                r'([A-Z]\w+(?:\s+[A-Z]\w+)*)\s+(?:appointed|named)\s+(\w+(?:\s+\w+)?)\s+(?:as\s+)?(?:CEO|director|head)',
            ],
            'located_in': [
                r'([A-Z]\w+(?:\s+[A-Z]\w+)*)\s+(?:is\s+)?(?:located|based|situated|headquartered)\s+in\s+([A-Z]\w+(?:\s+[A-Z]\w+)*)',
                r'([A-Z]\w+(?:\s+[A-Z]\w+)*)\s+office\s+(?:in|at)\s+([A-Z]\w+(?:\s+[A-Z]\w+)*)',
            ],
            'owns': [
                r'(\w+(?:\s+\w+)?)\s+(?:owns|founded|established)\s+([A-Z]\w+(?:\s+[A-Z]\w+)*)',
                r'([A-Z]\w+(?:\s+[A-Z]\w+)*)\s+(?:owned|founded)\s+by\s+(\w+(?:\s+\w+)?)',
            ],
            'competes_with': [
                r'([A-Z]\w+(?:\s+[A-Z]\w+)*)\s+(?:competes|compete)\s+with\s+([A-Z]\w+(?:\s+[A-Z]\w+)*)',
                r'([A-Z]\w+(?:\s+[A-Z]\w+)*)\s+vs\s+(?:vs\.?|versus)\s+([A-Z]\w+(?:\s+[A-Z]\w+)*)',
            ],
            'partners_with': [
                r'([A-Z]\w+(?:\s+[A-Z]\w+)*)\s+(?:partnered|partners|partnership)\s+with\s+([A-Z]\w+(?:\s+[A-Z]\w+)*)',
            ],
        }
    
    def _extract_by_regex(
        self,
        text: str,
        entities: Dict[str, List[Any]]
    ) -> List[ExtractedRelation]:
        """Ð˜Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ðµ Ð¾Ñ‚Ð½Ð¾ÑˆÐµÐ½Ð¸Ð¹ Ñ‡ÐµÑ€ÐµÐ· regex Ð¿Ð°Ñ‚Ñ‚ÐµÑ€Ð½Ñ‹"""
        relations = []
        
        for rel_type, patterns in self.regex_patterns.items():
            for pattern in patterns:
                try:
                    matches = re.finditer(pattern, text, re.IGNORECASE)
                    for match in matches:
                        if len(match.groups()) >= 2:
                            source = match.group(1).strip()
                            target = match.group(2).strip()
                            
                            # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ Ñ‡Ñ‚Ð¾ ÑÑƒÑ‰Ð½Ð¾ÑÑ‚Ð¸ ÐµÑÑ‚ÑŒ Ð² Ñ‚ÐµÐºÑÑ‚Ðµ
                            if self._is_valid_entity_pair(
                                source, target, entities
                            ):
                                relations.append(ExtractedRelation(
                                    source_entity=source,
                                    target_entity=target,
                                    relation_type=rel_type.upper(),
                                    confidence=0.85,
                                    evidence=match.group(0),
                                    source_method='regex'
                                ))
                except Exception as e:
                    logger.debug(f"Regex pattern error: {e}")
        
        return relations
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ÐœÐ•Ð¢ÐžÐ” 2: SPACY DEPENDENCY PARSING
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def _extract_by_spacy(
        self,
        text: str,
        entities: Dict[str, List[Any]]
    ) -> List[ExtractedRelation]:
        """Ð˜Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ðµ Ð¾Ñ‚Ð½Ð¾ÑˆÐµÐ½Ð¸Ð¹ Ñ‡ÐµÑ€ÐµÐ· spaCy dependency parsing"""
        if not self.spacy_nlp:
            return []
        
        relations = []
        doc = self.spacy_nlp(text)
        
        # Ð˜Ð·Ð²Ð»ÐµÐºÐ°ÐµÐ¼ Ð¸Ð¼ÐµÐ½Ð¾Ð²Ð°Ð½Ð½Ñ‹Ðµ ÑÑƒÑ‰Ð½Ð¾ÑÑ‚Ð¸ Ð¸Ð· spaCy
        entity_spans = {}
        for ent in doc.ents:
            if ent.label_ in ["PERSON", "ORG", "GPE"]:
                if ent.label_ not in entity_spans:
                    entity_spans[ent.label_] = []
                entity_spans[ent.label_].append((ent.text, ent.start, ent.end))
        
        # Ð˜Ñ‰ÐµÐ¼ Ð¾Ñ‚Ð½Ð¾ÑˆÐµÐ½Ð¸Ñ Ð¼ÐµÐ¶Ð´Ñƒ ÑÑƒÑ‰Ð½Ð¾ÑÑ‚ÑÐ¼Ð¸
        for token in doc:
            # WORKS_FOR: nsubj -> verb -> pobj/dobj
            if token.dep_ in ["nsubj", "nsubjpass"] and token.pos_ == "NOUN":
                verb = token.head
                
                # Ð˜Ñ‰ÐµÐ¼ Ð¾Ð±ÑŠÐµÐºÑ‚ Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ñ
                for child in verb.children:
                    if child.dep_ in ["pobj", "dobj", "attr"]:
                        relation_text = f"{token.text} {verb.text} {child.text}"
                        
                        relations.append(ExtractedRelation(
                            source_entity=token.text,
                            target_entity=child.text,
                            relation_type="WORKS_FOR",
                            confidence=0.72,
                            evidence=relation_text,
                            source_method='spacy'
                        ))
            
            # LOCATED_IN: compound nouns + prep
            if token.dep_ == "compound" and token.head.dep_ == "pobj":
                prep_token = token.head.head
                if prep_token.text.lower() in ["in", "at", "near"]:
                    relations.append(ExtractedRelation(
                        source_entity=f"{token.text} {token.head.text}",
                        target_entity=token.head.text,
                        relation_type="LOCATED_IN",
                        confidence=0.70,
                        evidence=f"{token.text} {token.head.text} {prep_token.text}",
                        source_method='spacy'
                    ))
        
        return relations
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ÐœÐ•Ð¢ÐžÐ” 3: BERT ZERO-SHOT CLASSIFICATION
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def _extract_by_bert(
        self,
        text: str,
        entities: Dict[str, List[Any]]
    ) -> List[ExtractedRelation]:
        """Ð˜Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ðµ Ð¾Ñ‚Ð½Ð¾ÑˆÐµÐ½Ð¸Ð¹ Ñ‡ÐµÑ€ÐµÐ· BERT zero-shot classification"""
        if not self.zeroshot_pipeline:
            return []
        
        relations = []
        
        # Ð¢Ð¸Ð¿Ñ‹ Ð¾Ñ‚Ð½Ð¾ÑˆÐµÐ½Ð¸Ð¹ Ð´Ð»Ñ ÐºÐ»Ð°ÑÑÐ¸Ñ„Ð¸ÐºÐ°Ñ†Ð¸Ð¸
        relation_types = [
            "WORKS_FOR",
            "OWNS",
            "LOCATED_IN",
            "MANAGES",
            "COMPETES_WITH",
            "PARTNERS_WITH",
            "INVOLVED_IN"
        ]
        
        # ÐÐ¾Ñ€Ð¼Ð°Ð»Ð¸Ð·ÑƒÐµÐ¼ ÑÑƒÑ‰Ð½Ð¾ÑÑ‚Ð¸
        persons = self._normalize_entities(entities.get('persons', []))
        orgs = self._normalize_entities(entities.get('organizations', []))
        locations = self._normalize_entities(entities.get('locations', []))
        
        # ÐšÐ»Ð°ÑÑÐ¸Ñ„Ð¸Ñ†Ð¸Ñ€ÑƒÐµÐ¼ Ð¿Ð°Ñ€Ñ‹ person-org
        for person in persons:
            for org in orgs:
                try:
                    hypothesis = f"{person} and {org}"
                    
                    result = self.zeroshot_pipeline(
                        hypothesis,
                        relation_types,
                        multi_class=False
                    )
                    
                    top_relation = result['labels'][0]
                    confidence = float(result['scores'][0])
                    
                    if confidence > 0.65:
                        relations.append(ExtractedRelation(
                            source_entity=person,
                            target_entity=org,
                            relation_type=top_relation,
                            confidence=confidence,
                            evidence=hypothesis,
                            source_method='bert'
                        ))
                except Exception as e:
                    logger.debug(f"BERT classification failed: {e}")
        
        # ÐšÐ»Ð°ÑÑÐ¸Ñ„Ð¸Ñ†Ð¸Ñ€ÑƒÐµÐ¼ Ð¿Ð°Ñ€Ñ‹ org-location
        for org in orgs:
            for loc in locations:
                try:
                    hypothesis = f"{org} in {loc}"
                    
                    result = self.zeroshot_pipeline(
                        hypothesis,
                        ["LOCATED_IN", "OTHER"],
                        multi_class=False
                    )
                    
                    confidence = float(result['scores'][0])
                    if result['labels'][0] == "LOCATED_IN" and confidence > 0.70:
                        relations.append(ExtractedRelation(
                            source_entity=org,
                            target_entity=loc,
                            relation_type="LOCATED_IN",
                            confidence=confidence,
                            evidence=hypothesis,
                            source_method='bert'
                        ))
                except Exception as e:
                    logger.debug(f"BERT classification failed: {e}")
        
        return relations
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Ð£Ð¢Ð˜Ð›Ð˜Ð¢Ð«
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def _normalize_entities(
        self,
        entities: List[Any]
    ) -> List[str]:
        """ÐÐ¾Ñ€Ð¼Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ ÑÑƒÑ‰Ð½Ð¾ÑÑ‚ÐµÐ¹ Ð¸Ð· Ñ€Ð°Ð·Ð½Ñ‹Ñ… Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚Ð¾Ð²"""
        normalized = []
        for entity in entities:
            if isinstance(entity, dict):
                if 'name' in entity:
                    normalized.append(entity['name'])
                elif 'text' in entity:
                    normalized.append(entity['text'])
            elif isinstance(entity, str):
                normalized.append(entity)
            else:
                # ÐŸÐ¾Ð¿Ñ€Ð¾Ð±ÑƒÐµÐ¼ Ð¿Ð¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ .name attribute
                if hasattr(entity, 'name'):
                    normalized.append(entity.name)
                elif hasattr(entity, 'text'):
                    normalized.append(entity.text)
        
        return list(set(normalized))  # Ð£Ð´Ð°Ð»ÑÐµÐ¼ Ð´ÑƒÐ±Ð»Ð¸ÐºÐ°Ñ‚Ñ‹
    
    def _is_valid_entity_pair(
        self,
        source: str,
        target: str,
        entities: Dict[str, List[Any]]
    ) -> bool:
        """ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ñ‡Ñ‚Ð¾ Ð¿Ð°Ñ€Ð° ÑÑƒÑ‰Ð½Ð¾ÑÑ‚ÐµÐ¹ Ñ€ÐµÐ°Ð»ÑŒÐ½Ð¾ ÑÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÐµÑ‚"""
        all_persons = self._normalize_entities(
            entities.get('persons', [])
        )
        all_orgs = self._normalize_entities(
            entities.get('organizations', [])
        )
        all_locations = self._normalize_entities(
            entities.get('locations', [])
        )
        all_entities = all_persons + all_orgs + all_locations
        
        # ÐÐµÑ‡ÐµÑ‚ÐºÐ¾Ðµ ÑÑ€Ð°Ð²Ð½ÐµÐ½Ð¸Ðµ (Ð±ÐµÐ· ÑƒÑ‡ÐµÑ‚Ð° Ñ€ÐµÐ³Ð¸ÑÑ‚Ñ€Ð° Ð¸ Ð¿Ñ€Ð¾Ð±ÐµÐ»Ð¾Ð²)
        source_normalized = source.lower().strip()
        target_normalized = target.lower().strip()
        
        for entity in all_entities:
            entity_normalized = entity.lower().strip()
            if (source_normalized in entity_normalized or 
                entity_normalized in source_normalized):
                for entity2 in all_entities:
                    entity2_normalized = entity2.lower().strip()
                    if (target_normalized in entity2_normalized or 
                        entity2_normalized in target_normalized):
                        return True
        
        return False
    
    def _deduplicate_relations(
        self,
        relations: List[ExtractedRelation]
    ) -> List[ExtractedRelation]:
        """Ð”ÐµÐ´ÑƒÐ¿Ð»Ð¸ÐºÐ°Ñ†Ð¸Ñ Ð¾Ñ‚Ð½Ð¾ÑˆÐµÐ½Ð¸Ð¹, Ð¾ÑÑ‚Ð°Ð²Ð»ÑÐµÐ¼ max confidence"""
        seen = {}
        
        for rel in relations:
            key = (
                rel.source_entity.lower(),
                rel.target_entity.lower(),
                rel.relation_type
            )
            
            if key not in seen:
                seen[key] = rel
            elif rel.confidence > seen[key].confidence:
                seen[key] = rel
        
        # Ð¤Ð¸Ð»ÑŒÑ‚Ñ€ÑƒÐµÐ¼ Ð¿Ð¾ Ð¼Ð¸Ð½Ð¸Ð¼Ð°Ð»ÑŒÐ½Ð¾Ð¹ ÑƒÐ²ÐµÑ€ÐµÐ½Ð½Ð¾ÑÑ‚Ð¸
        return [
            rel for rel in seen.values()
            if rel.confidence >= 0.60
        ]
